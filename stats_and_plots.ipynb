{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Database  Number of Tables  Number of Columns\n",
      "0   debit_card_specializing                 6                 23\n",
      "1                 financial                 8                 55\n",
      "2                 formula_1                14                 96\n",
      "3        california_schools                 3                 89\n",
      "4                card_games                 7                117\n",
      "5       european_football_2                 8                201\n",
      "6     thrombosis_prediction                 3                 64\n",
      "7                toxicology                 4                 11\n",
      "8              student_club                 8                 48\n",
      "9                 superhero                10                 31\n",
      "10       codebase_community                 8                 71\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the main folder containing subfolders with databases\n",
    "main_folder = 'data/original_dev/dev_databases'\n",
    "\n",
    "# List all subfolders in the main folder\n",
    "databases = [d for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each database subfolder\n",
    "for db_name in databases:\n",
    "    db_path = os.path.join(main_folder, db_name, f\"{db_name}.sqlite\")\n",
    "    \n",
    "    # Check if the SQLite file exists\n",
    "    if os.path.exists(db_path):\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the list of tables\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        num_tables = len(tables)\n",
    "        \n",
    "        # Initialize the count of columns\n",
    "        num_columns = 0\n",
    "        \n",
    "        # Loop through each table to count columns\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info(\\\"{table_name}\\\");\")\n",
    "            columns = cursor.fetchall()\n",
    "            num_columns += len(columns)\n",
    "        \n",
    "        # Add the results to the list\n",
    "        results.append({\n",
    "            'Database': db_name,\n",
    "            'Number of Tables': num_tables,\n",
    "            'Number of Columns': num_columns\n",
    "        })\n",
    "        \n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n",
    "# Convert the results to a DataFrame and display it\n",
    "df_results = pd.DataFrame(results)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Database & Number of Tables & Number of Columns \\\\\n",
      "\\midrule\n",
      "debit_card_specializing & 5 & 21 \\\\\n",
      "financial & 8 & 55 \\\\\n",
      "formula_1 & 13 & 94 \\\\\n",
      "california_schools & 3 & 89 \\\\\n",
      "card_games & 6 & 115 \\\\\n",
      "european_football_2 & 7 & 199 \\\\\n",
      "thrombosis_prediction & 3 & 64 \\\\\n",
      "toxicology & 4 & 11 \\\\\n",
      "student_club & 8 & 48 \\\\\n",
      "superhero & 10 & 31 \\\\\n",
      "codebase_community & 8 & 71 \\\\\n",
      "Total & 75 & 798 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List all subfolders in the main folder\n",
    "databases = [d for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each database subfolder\n",
    "for db_name in databases:\n",
    "    db_path = os.path.join(main_folder, db_name, f\"{db_name}.sqlite\")\n",
    "    \n",
    "    # Check if the SQLite file exists\n",
    "    if os.path.exists(db_path):\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the list of tables, excluding 'sqlite_sequence'\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'sqlite_sequence';\")\n",
    "        tables = cursor.fetchall()\n",
    "        num_tables = len(tables)\n",
    "        \n",
    "        # Initialize the count of columns\n",
    "        num_columns = 0\n",
    "        \n",
    "        # Loop through each table to count columns\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info(\\\"{table_name}\\\");\")\n",
    "            columns = cursor.fetchall()\n",
    "            num_columns += len(columns)\n",
    "        \n",
    "        # Add the results to the list\n",
    "        results.append({\n",
    "            'Database': db_name,\n",
    "            'Number of Tables': num_tables,\n",
    "            'Number of Columns': num_columns\n",
    "        })\n",
    "        \n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate totals\n",
    "total_tables = df['Number of Tables'].sum()\n",
    "total_columns = df['Number of Columns'].sum()\n",
    "\n",
    "# Append the totals row to the DataFrame\n",
    "totals_row = pd.DataFrame([['Total', total_tables, total_columns]], columns=df.columns)\n",
    "df_with_totals = pd.concat([df, totals_row], ignore_index=True)\n",
    "\n",
    "# Convert to LaTeX table with totals\n",
    "latex_table_with_totals = df_with_totals.to_latex(index=False)\n",
    "\n",
    "# Print LaTeX table\n",
    "print(latex_table_with_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Final difficulty</th>\n",
       "      <th>Easy</th>\n",
       "      <th>Hard</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Very Hard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>database_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>california_schools</th>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_games</th>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codebase_community</th>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debit_card_specializing</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>european_football_2</th>\n",
       "      <td>34</td>\n",
       "      <td>103</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formula_1</th>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_club</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superhero</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thrombosis_prediction</th>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxicology</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Final difficulty         Easy  Hard  Medium  Very Hard\n",
       "database_name                                         \n",
       "california_schools         24    30      32          3\n",
       "card_games                 37    50      28          0\n",
       "codebase_community         62     5       4          0\n",
       "debit_card_specializing    17     1       3          0\n",
       "european_football_2        34   103      62          0\n",
       "financial                  21    11       9         13\n",
       "formula_1                  49    10      35          0\n",
       "student_club               39     0       9          0\n",
       "superhero                  30     0       1          0\n",
       "thrombosis_prediction      56     6       2          0\n",
       "toxicology                  8     2       0          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = 'annotations/Difficulty Annotation.xlsx'\n",
    "\n",
    "# Read the specific sheet from the Excel file\n",
    "df = pd.read_excel(file_path, sheet_name='Sammanställning')\n",
    "\n",
    "# Group by database_name and Final difficulty, and count occurrences\n",
    "difficulty_counts = df.groupby(['database_name', 'Final difficulty']).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "\n",
    "difficulty_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotator 1</th>\n",
       "      <th>Annotator 2</th>\n",
       "      <th>Mean Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annotation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No description</th>\n",
       "      <td>18.272841</td>\n",
       "      <td>18.272841</td>\n",
       "      <td>18.272841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I can't tell</th>\n",
       "      <td>1.501877</td>\n",
       "      <td>0.125156</td>\n",
       "      <td>0.813517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incorrect</th>\n",
       "      <td>2.628285</td>\n",
       "      <td>1.126408</td>\n",
       "      <td>1.877347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somewhat correct</th>\n",
       "      <td>40.801001</td>\n",
       "      <td>36.795995</td>\n",
       "      <td>38.798498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Almost Perfect</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perfect</th>\n",
       "      <td>36.795995</td>\n",
       "      <td>43.679599</td>\n",
       "      <td>40.237797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Annotator 1  Annotator 2  Mean Percentage\n",
       "Annotation                                                 \n",
       "No description      18.272841    18.272841        18.272841\n",
       "I can't tell         1.501877     0.125156         0.813517\n",
       "Incorrect            2.628285     1.126408         1.877347\n",
       "Somewhat correct    40.801001    36.795995        38.798498\n",
       "Almost Perfect       0.000000     0.000000         0.000000\n",
       "Perfect             36.795995    43.679599        40.237797"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = 'annotations/Original_BIRD_Annotations2.xlsx'\n",
    "\n",
    "# Read the sheets for each annotator\n",
    "df_axel = pd.read_excel(file_path, sheet_name='Axel')\n",
    "df_erik = pd.read_excel(file_path, sheet_name='Erik')\n",
    "\n",
    "# Define all quality classes\n",
    "quality_classes = [\"No description\", \"I can't tell\", \"Incorrect\", \"Somewhat correct\", \"Almost Perfect\", \"Perfect\"]\n",
    "\n",
    "# Calculate the percentage of each quality class for Axel\n",
    "axel_counts = df_axel['Annotation'].value_counts(normalize=True) * 100\n",
    "axel_counts = axel_counts.reindex(quality_classes).fillna(0)\n",
    "\n",
    "# Calculate the percentage of each quality class for Erik\n",
    "erik_counts = df_erik['Annotation'].value_counts(normalize=True) * 100\n",
    "erik_counts = erik_counts.reindex(quality_classes).fillna(0)\n",
    "\n",
    "# Combine the counts into a single DataFrame\n",
    "combined_counts = pd.DataFrame({\n",
    "    'Annotator 1': axel_counts,\n",
    "    'Annotator 2': erik_counts\n",
    "})\n",
    "\n",
    "\n",
    "# Calculate the mean percentage for each quality class\n",
    "combined_counts['Mean Percentage'] = combined_counts.mean(axis=1)\n",
    "\n",
    "\n",
    "combined_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     Forsberg     Oscar\n",
       " Annotation LLM 3                       \n",
       " 4. Perfect           0.775689  0.723058\n",
       " No description       0.068922  0.068922\n",
       " 3. Almost perfect    0.062657  0.100251\n",
       " 2. Somewhat correct  0.050125  0.072682\n",
       " 1. Incorrect         0.042607  0.035088,\n",
       " Annotation LLM 3\n",
       " 4. Perfect             74.937343\n",
       " No description          6.892231\n",
       " 3. Almost perfect       8.145363\n",
       " 2. Somewhat correct     6.140351\n",
       " 1. Incorrect            3.884712\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'annotations/LLM Prediction Annotations(11).xlsx'\n",
    "sheets = ['Forsberg', 'Oscar']\n",
    "\n",
    "# Read the sheets into separate dataframes\n",
    "df_hjalmar = pd.read_excel(file_path, sheet_name=sheets[0])\n",
    "df_sorme = pd.read_excel(file_path, sheet_name=sheets[1])\n",
    "\n",
    "# Extract the annotation columns\n",
    "annotations_hjalmar = df_hjalmar['Annotation LLM 3']\n",
    "annotations_sorme = df_sorme['Annotation LLM 3']\n",
    "\n",
    "# Calculate the mean number of annotations in each class\n",
    "mean_annotations_hjalmar = annotations_hjalmar.value_counts(normalize=True)\n",
    "mean_annotations_sorme = annotations_sorme.value_counts(normalize=True)\n",
    "\n",
    "# Combine the means into a single dataframe\n",
    "mean_annotations = pd.concat([mean_annotations_hjalmar, mean_annotations_sorme], axis=1)\n",
    "mean_annotations.columns = ['Forsberg', 'Oscar']\n",
    "\n",
    "# Calculate the overall percentage between the classes\n",
    "overall_percentage = mean_annotations.mean(axis=1) * 100\n",
    "\n",
    "# Display the results\n",
    "mean_annotations, overall_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to output/GOLD_DATASET_FINAL_with_difficulties.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"output/GOLD_DATASET_FINAL.csv\"\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file_path = \"annotations/Difficulty Annotation.xlsx\"\n",
    "excel_data = pd.read_excel(excel_file_path, sheet_name=\"Sammanställning\")\n",
    "\n",
    "# Append the 'Final difficulty' column to the CSV data\n",
    "csv_data['Final difficulty'] = excel_data['Final difficulty']\n",
    "\n",
    "# Save the new dataframe to a new CSV file\n",
    "output_file_path = \"output/GOLD_DATASET_FINAL_with_difficulties.csv\"\n",
    "csv_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Annotation 1 LLM 1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sql_description_gen/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Annotation 1 LLM 1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert the annotations to numeric values\u001b[39;00m\n\u001b[1;32m      9\u001b[0m annotation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. Perfect\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Almost perfect\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Somewhat correct\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 16\u001b[0m data_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation 1 LLM 1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAnnotation 1 LLM 1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(annotation_mapping)\n\u001b[1;32m     17\u001b[0m data_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation 2 LLM 1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation 2 LLM 1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(annotation_mapping)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the average annotation per difficulty\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sql_description_gen/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/sql_description_gen/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Annotation 1 LLM 1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('annotations/LLM Prediction Annotations(12).xlsx', sheet_name=\"Scores\")\n",
    "\n",
    "# Filter the relevant columns\n",
    "data_filtered = data[['Annotation 1 LLM 2', 'Annotation 2 LLM 2', 'Final difficulty']]\n",
    "\n",
    "# Convert the annotations to numeric values\n",
    "annotation_mapping = {\n",
    "    \"4. Perfect\": 4,\n",
    "    \"3. Almost perfect\": 3,\n",
    "    \"2. Somewhat correct\": 2,\n",
    "    \"1. Incorrect\": 1\n",
    "}\n",
    "\n",
    "data_filtered['Annotation 1 LLM 1'] = data_filtered['Annotation 1 LLM 1'].map(annotation_mapping)\n",
    "data_filtered['Annotation 2 LLM 1'] = data_filtered['Annotation 2 LLM 1'].map(annotation_mapping)\n",
    "\n",
    "# Calculate the average annotation per difficulty\n",
    "data_filtered['Average Quality'] = data_filtered[['Annotation 1 LLM 1', 'Annotation 2 LLM 1']].mean(axis=1)\n",
    "\n",
    "# Group by difficulty and calculate the mean quality\n",
    "difficulty_quality = data_filtered.groupby('Final difficulty')['Average Quality'].mean().reset_index()\n",
    "\n",
    "latex_table = difficulty_quality.to_latex(index=False)\n",
    "\n",
    "# Display the LaTeX table code\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_description_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
